[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /opt/airflow/dags

# Hostname by providing a path to a callable, which will resolve the hostname.
hostname_callable = airflow.utils.net.getfqdn

# Default timezone in case supplied date times are naive
default_timezone = utc

# The executor class that airflow should use.
executor = LocalExecutor

# This defines the maximum number of task instances that can run concurrently per scheduler
parallelism = 32

# The maximum number of task instances allowed to run concurrently in each DAG
max_active_tasks_per_dag = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Secret key to save connection passwords in the db
fernet_key = 

# Whether to disable pickling dags
donot_pickle = True

# How long before timing out a python file import
dagbag_import_timeout = 30.0

# Should a traceback be shown in the UI for dagbag import errors
dagbag_import_error_tracebacks = True

# If tracebacks are shown, how many entries from the traceback should be shown
dagbag_import_error_traceback_depth = 2

# How long before timing out a DagFileProcessor
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# Whether to enable pickling for xcom
enable_xcom_pickling = False

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf
dag_run_conf_overrides_params = True

# The number of retries each task is going to have by default
default_task_retries = 0

# The number of seconds each task is going to wait by default between retries
default_task_retry_delay = 300

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate
min_serialized_dag_update_interval = 30

# If True, serialized DAGs are compressed before writing to DB
compress_serialized_dags = False

# Fetching serialized DAG can not be faster than a minimum interval to reduce database read rate
min_serialized_dag_fetch_interval = 10

# Maximum number of Rendered Task Instance Fields (Template Fields) per task to store
max_num_rendered_ti_fields_per_task = 30

# On each dagrun check against defined SLAs
check_slas = True

# Task Slot counts for default_pool
default_pool_task_slot_count = 128

# The maximum list/dict length an XCom can push to trigger task mapping
max_map_length = 1024

pythonpath = /opt/airflow/scripts

[database]
# The SQLAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The SQLAlchemy pool size is the maximum number of database connections
sql_alchemy_pool_size = 10

# The maximum overflow size of the pool
sql_alchemy_max_overflow = 20

# The SQLAlchemy pool recycle is the number of seconds a connection
sql_alchemy_pool_recycle = 3600

# Check connection at the start of each connection pool checkout
sql_alchemy_pool_pre_ping = True

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Logging level for celery
celery_logging_level = WARNING

# Logging level for Flask-appbuilder UI
fab_logging_level = WARNING

# Format of Log line
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Where to send dag parser logs
dag_processor_log_target = file

# Format of Dag Processor Log line
dag_processor_log_format = [%%(asctime)s] [SOURCE:DAG_PROCESSOR] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Full path of dag_processor_manager logfile
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Formatting for how airflow generates file names/paths for each task run
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log

# Formatting for how airflow generates file names for log
log_processor_filename_template = {{{{ filename }}}}.log

# Name of handler to read task instance logs
task_log_reader = task

[metrics]
# Enables sending metrics to StatsD
statsd_on = False

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Controls how long the scheduler will sleep between loops
scheduler_idle_sleep_time = 1

# Number of seconds after which a DAG file is parsed
min_file_process_interval = 30

# How often to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduler heartbeat happened more than this many seconds ago, scheduler is considered unhealthy
scheduler_health_check_threshold = 30

# How often should pool usage stats be sent to StatsD
pool_metrics_interval = 5.0

# Turn off scheduler catchup by setting this to False
catchup_by_default = False

# This changes the batch size of queries in the scheduling main loop
max_tis_per_query = 16

# Should the scheduler issue SELECT ... FOR UPDATE in relevant queries
use_row_level_locking = True

# Max number of DAGs to create DagRuns for per scheduler loop
max_dagruns_to_create_per_loop = 10

# How many DagRuns should a scheduler examine when scheduling and queuing tasks
max_dagruns_per_loop_to_schedule = 20

# Should the Task supervisor process perform a "mini scheduler"
schedule_after_task_execution = True

# The scheduler can run multiple processes in parallel to parse dags
parsing_processes = 2

# One of modified_time, random_seeded_by_host and alphabetical
file_parsing_sort_mode = modified_time

# Turn off scheduler use of cron intervals
use_job_schedule = True

[webserver]
# The base url of your website
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 500

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 500

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = generated_password_with_at_least_16_characters

# Number of workers to run the Gunicorn web server
workers = 4

# Log files for the gunicorn webserver
access_logfile = -

# Log files for the gunicorn webserver
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Default DAG view
dag_default_view = grid

# Default DAG orientation
dag_orientation = LR

# The amount of time webserver will wait for initial handshake while fetching logs
log_fetch_timeout_sec = 5

# Time interval to wait before next log fetching
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Define the color of navigation bar
navbar_color = #fff

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite = Lax

# Default setting for wrap toggle on DAG code and TI log views
default_wrap = False

# How frequently the DAG data will auto-refresh in graph or grid view when auto-refresh is turned on
auto_refresh_interval = 3

[lineage]
# what lineage backend to use
backend =

[operators]
# The default owner assigned to each new operator
default_owner = airflow

# Indicates the default number of CPU units allocated to each operator
default_cpus = 1

# Indicates the default number of RAM allocated to each operator
default_ram = 512

# Indicates the default number of disk storage allocated to each operator
default_disk = 512

# Indicates the default number of GPUs allocated to each operator
default_gpus = 0

# Default queue that tasks get assigned to and that worker listen on
default_queue = default

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com

[celery]
# This section only applies if you are using the CeleryExecutor
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_prefetch_multiplier = 1
worker_enable_remote_control = true
broker_url = redis://redis:6379/0
result_backend = db+postgresql://postgres:airflow@postgres/airflow
flower_host = 0.0.0.0
flower_port = 5555
sync_parallelism = 0
ssl_active = False
task_acks_late = True
task_track_started = True

[kubernetes]
# Kubernetes configs